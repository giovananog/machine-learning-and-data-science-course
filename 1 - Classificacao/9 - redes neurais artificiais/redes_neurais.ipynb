{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSNUSSeOhqAd"
      },
      "source": [
        "# Redes neurais artificiais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvdcLLQLnFIW"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foQGPam1hrwG"
      },
      "source": [
        "## Base credit data - 99.80%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6uL7hyzm8Dd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('credit.pkl', 'rb') as f:\n",
        "  X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIVKNjPom-Dr",
        "outputId": "89fb5c71-7134-44fa-c849-1c9aa79f6f57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "execution_count": 300,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_credit_treinamento.shape, y_credit_treinamento.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df5EfDyjnAB6",
        "outputId": "51412d26-05a6-41dd-8937-e8e91c223161"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "execution_count": 301,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_credit_teste.shape, y_credit_teste.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c7fjRs-qePa",
        "outputId": "8c5a98cb-5078-4991-ffbe-908cd58a6406"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "execution_count": 302,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(3 + 1) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCGNa_d8nYOf",
        "outputId": "62e326f1-1841-4386-90c2-cb97a5458876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.69563102\n",
            "Iteration 2, loss = 0.64199931\n",
            "Iteration 3, loss = 0.59739529\n",
            "Iteration 4, loss = 0.55861946\n",
            "Iteration 5, loss = 0.52415105\n",
            "Iteration 6, loss = 0.49268361\n",
            "Iteration 7, loss = 0.46342175\n",
            "Iteration 8, loss = 0.43565040\n",
            "Iteration 9, loss = 0.40861794\n",
            "Iteration 10, loss = 0.38197692\n",
            "Iteration 11, loss = 0.35631328\n",
            "Iteration 12, loss = 0.33223738\n",
            "Iteration 13, loss = 0.30960538\n",
            "Iteration 14, loss = 0.28868836\n",
            "Iteration 15, loss = 0.26879630\n",
            "Iteration 16, loss = 0.25040226\n",
            "Iteration 17, loss = 0.23353503\n",
            "Iteration 18, loss = 0.21834154\n",
            "Iteration 19, loss = 0.20414498\n",
            "Iteration 20, loss = 0.19187713\n",
            "Iteration 21, loss = 0.18034719\n",
            "Iteration 22, loss = 0.17013913\n",
            "Iteration 23, loss = 0.16090659\n",
            "Iteration 24, loss = 0.15265189\n",
            "Iteration 25, loss = 0.14517457\n",
            "Iteration 26, loss = 0.13862123\n",
            "Iteration 27, loss = 0.13267472\n",
            "Iteration 28, loss = 0.12730252\n",
            "Iteration 29, loss = 0.12242248\n",
            "Iteration 30, loss = 0.11802243\n",
            "Iteration 31, loss = 0.11393696\n",
            "Iteration 32, loss = 0.11035122\n",
            "Iteration 33, loss = 0.10717059\n",
            "Iteration 34, loss = 0.10415624\n",
            "Iteration 35, loss = 0.10141304\n",
            "Iteration 36, loss = 0.09898186\n",
            "Iteration 37, loss = 0.09639900\n",
            "Iteration 38, loss = 0.09422499\n",
            "Iteration 39, loss = 0.09212882\n",
            "Iteration 40, loss = 0.09015680\n",
            "Iteration 41, loss = 0.08833770\n",
            "Iteration 42, loss = 0.08653411\n",
            "Iteration 43, loss = 0.08479415\n",
            "Iteration 44, loss = 0.08318081\n",
            "Iteration 45, loss = 0.08172366\n",
            "Iteration 46, loss = 0.08021279\n",
            "Iteration 47, loss = 0.07887650\n",
            "Iteration 48, loss = 0.07734999\n",
            "Iteration 49, loss = 0.07603770\n",
            "Iteration 50, loss = 0.07485928\n",
            "Iteration 51, loss = 0.07359342\n",
            "Iteration 52, loss = 0.07241261\n",
            "Iteration 53, loss = 0.07113493\n",
            "Iteration 54, loss = 0.06990945\n",
            "Iteration 55, loss = 0.06874182\n",
            "Iteration 56, loss = 0.06730819\n",
            "Iteration 57, loss = 0.06599520\n",
            "Iteration 58, loss = 0.06474169\n",
            "Iteration 59, loss = 0.06339406\n",
            "Iteration 60, loss = 0.06236851\n",
            "Iteration 61, loss = 0.06097315\n",
            "Iteration 62, loss = 0.05996020\n",
            "Iteration 63, loss = 0.05896209\n",
            "Iteration 64, loss = 0.05786071\n",
            "Iteration 65, loss = 0.05694569\n",
            "Iteration 66, loss = 0.05600704\n",
            "Iteration 67, loss = 0.05514040\n",
            "Iteration 68, loss = 0.05417726\n",
            "Iteration 69, loss = 0.05343125\n",
            "Iteration 70, loss = 0.05256729\n",
            "Iteration 71, loss = 0.05174548\n",
            "Iteration 72, loss = 0.05098241\n",
            "Iteration 73, loss = 0.05013997\n",
            "Iteration 74, loss = 0.04942423\n",
            "Iteration 75, loss = 0.04885300\n",
            "Iteration 76, loss = 0.04798777\n",
            "Iteration 77, loss = 0.04730225\n",
            "Iteration 78, loss = 0.04679511\n",
            "Iteration 79, loss = 0.04603224\n",
            "Iteration 80, loss = 0.04534835\n",
            "Iteration 81, loss = 0.04463814\n",
            "Iteration 82, loss = 0.04399022\n",
            "Iteration 83, loss = 0.04344426\n",
            "Iteration 84, loss = 0.04285297\n",
            "Iteration 85, loss = 0.04227912\n",
            "Iteration 86, loss = 0.04167021\n",
            "Iteration 87, loss = 0.04114668\n",
            "Iteration 88, loss = 0.04052731\n",
            "Iteration 89, loss = 0.04004709\n",
            "Iteration 90, loss = 0.03942249\n",
            "Iteration 91, loss = 0.03900225\n",
            "Iteration 92, loss = 0.03845060\n",
            "Iteration 93, loss = 0.03804958\n",
            "Iteration 94, loss = 0.03747574\n",
            "Iteration 95, loss = 0.03705860\n",
            "Iteration 96, loss = 0.03652939\n",
            "Iteration 97, loss = 0.03602683\n",
            "Iteration 98, loss = 0.03556547\n",
            "Iteration 99, loss = 0.03519259\n",
            "Iteration 100, loss = 0.03471345\n",
            "Iteration 101, loss = 0.03419047\n",
            "Iteration 102, loss = 0.03381431\n",
            "Iteration 103, loss = 0.03333985\n",
            "Iteration 104, loss = 0.03294446\n",
            "Iteration 105, loss = 0.03252561\n",
            "Iteration 106, loss = 0.03220166\n",
            "Iteration 107, loss = 0.03161357\n",
            "Iteration 108, loss = 0.03122787\n",
            "Iteration 109, loss = 0.03087067\n",
            "Iteration 110, loss = 0.03040846\n",
            "Iteration 111, loss = 0.03027931\n",
            "Iteration 112, loss = 0.02969753\n",
            "Iteration 113, loss = 0.02959281\n",
            "Iteration 114, loss = 0.02899508\n",
            "Iteration 115, loss = 0.02866885\n",
            "Iteration 116, loss = 0.02850542\n",
            "Iteration 117, loss = 0.02823450\n",
            "Iteration 118, loss = 0.02778357\n",
            "Iteration 119, loss = 0.02745708\n",
            "Iteration 120, loss = 0.02713565\n",
            "Iteration 121, loss = 0.02684208\n",
            "Iteration 122, loss = 0.02663336\n",
            "Iteration 123, loss = 0.02626788\n",
            "Iteration 124, loss = 0.02607934\n",
            "Iteration 125, loss = 0.02568176\n",
            "Iteration 126, loss = 0.02538577\n",
            "Iteration 127, loss = 0.02521144\n",
            "Iteration 128, loss = 0.02489558\n",
            "Iteration 129, loss = 0.02460481\n",
            "Iteration 130, loss = 0.02431082\n",
            "Iteration 131, loss = 0.02401302\n",
            "Iteration 132, loss = 0.02371768\n",
            "Iteration 133, loss = 0.02354522\n",
            "Iteration 134, loss = 0.02324261\n",
            "Iteration 135, loss = 0.02295695\n",
            "Iteration 136, loss = 0.02271277\n",
            "Iteration 137, loss = 0.02251033\n",
            "Iteration 138, loss = 0.02230054\n",
            "Iteration 139, loss = 0.02202537\n",
            "Iteration 140, loss = 0.02181097\n",
            "Iteration 141, loss = 0.02155395\n",
            "Iteration 142, loss = 0.02130890\n",
            "Iteration 143, loss = 0.02115675\n",
            "Iteration 144, loss = 0.02095499\n",
            "Iteration 145, loss = 0.02080527\n",
            "Iteration 146, loss = 0.02055449\n",
            "Iteration 147, loss = 0.02020430\n",
            "Iteration 148, loss = 0.02017612\n",
            "Iteration 149, loss = 0.01997200\n",
            "Iteration 150, loss = 0.01991583\n",
            "Iteration 151, loss = 0.01963105\n",
            "Iteration 152, loss = 0.01939818\n",
            "Iteration 153, loss = 0.01926611\n",
            "Iteration 154, loss = 0.01906530\n",
            "Iteration 155, loss = 0.01897560\n",
            "Iteration 156, loss = 0.01871979\n",
            "Iteration 157, loss = 0.01857384\n",
            "Iteration 158, loss = 0.01838620\n",
            "Iteration 159, loss = 0.01822246\n",
            "Iteration 160, loss = 0.01810623\n",
            "Iteration 161, loss = 0.01799767\n",
            "Iteration 162, loss = 0.01781558\n",
            "Iteration 163, loss = 0.01762274\n",
            "Iteration 164, loss = 0.01740221\n",
            "Iteration 165, loss = 0.01735463\n",
            "Iteration 166, loss = 0.01708736\n",
            "Iteration 167, loss = 0.01705619\n",
            "Iteration 168, loss = 0.01701242\n",
            "Iteration 169, loss = 0.01671360\n",
            "Iteration 170, loss = 0.01670004\n",
            "Iteration 171, loss = 0.01647192\n",
            "Iteration 172, loss = 0.01638068\n",
            "Iteration 173, loss = 0.01624162\n",
            "Iteration 174, loss = 0.01606447\n",
            "Iteration 175, loss = 0.01620046\n",
            "Iteration 176, loss = 0.01592600\n",
            "Iteration 177, loss = 0.01575832\n",
            "Iteration 178, loss = 0.01560869\n",
            "Iteration 179, loss = 0.01545388\n",
            "Iteration 180, loss = 0.01538643\n",
            "Iteration 181, loss = 0.01524260\n",
            "Iteration 182, loss = 0.01524223\n",
            "Iteration 183, loss = 0.01501152\n",
            "Iteration 184, loss = 0.01500906\n",
            "Iteration 185, loss = 0.01485299\n",
            "Iteration 186, loss = 0.01474207\n",
            "Iteration 187, loss = 0.01475671\n",
            "Iteration 188, loss = 0.01447220\n",
            "Iteration 189, loss = 0.01441952\n",
            "Iteration 190, loss = 0.01430936\n",
            "Iteration 191, loss = 0.01425552\n",
            "Iteration 192, loss = 0.01404934\n",
            "Iteration 193, loss = 0.01414359\n",
            "Iteration 194, loss = 0.01396411\n",
            "Iteration 195, loss = 0.01381940\n",
            "Iteration 196, loss = 0.01375314\n",
            "Iteration 197, loss = 0.01372804\n",
            "Iteration 198, loss = 0.01375760\n",
            "Iteration 199, loss = 0.01336520\n",
            "Iteration 200, loss = 0.01349729\n",
            "Iteration 201, loss = 0.01343407\n",
            "Iteration 202, loss = 0.01316064\n",
            "Iteration 203, loss = 0.01305272\n",
            "Iteration 204, loss = 0.01292789\n",
            "Iteration 205, loss = 0.01286101\n",
            "Iteration 206, loss = 0.01291647\n",
            "Iteration 207, loss = 0.01268304\n",
            "Iteration 208, loss = 0.01279738\n",
            "Iteration 209, loss = 0.01255238\n",
            "Iteration 210, loss = 0.01252475\n",
            "Iteration 211, loss = 0.01240479\n",
            "Iteration 212, loss = 0.01247991\n",
            "Iteration 213, loss = 0.01219937\n",
            "Iteration 214, loss = 0.01223160\n",
            "Iteration 215, loss = 0.01221392\n",
            "Iteration 216, loss = 0.01200824\n",
            "Iteration 217, loss = 0.01192054\n",
            "Iteration 218, loss = 0.01186458\n",
            "Iteration 219, loss = 0.01177952\n",
            "Iteration 220, loss = 0.01176834\n",
            "Iteration 221, loss = 0.01165068\n",
            "Iteration 222, loss = 0.01161677\n",
            "Iteration 223, loss = 0.01149361\n",
            "Iteration 224, loss = 0.01175609\n",
            "Iteration 225, loss = 0.01187955\n",
            "Iteration 226, loss = 0.01147827\n",
            "Iteration 227, loss = 0.01120215\n",
            "Iteration 228, loss = 0.01123971\n",
            "Iteration 229, loss = 0.01128287\n",
            "Iteration 230, loss = 0.01093899\n",
            "Iteration 231, loss = 0.01109925\n",
            "Iteration 232, loss = 0.01095156\n",
            "Iteration 233, loss = 0.01083561\n",
            "Iteration 234, loss = 0.01087393\n",
            "Iteration 235, loss = 0.01081751\n",
            "Iteration 236, loss = 0.01082183\n",
            "Iteration 237, loss = 0.01074855\n",
            "Iteration 238, loss = 0.01056398\n",
            "Iteration 239, loss = 0.01052031\n",
            "Iteration 240, loss = 0.01043008\n",
            "Iteration 241, loss = 0.01045711\n",
            "Iteration 242, loss = 0.01034799\n",
            "Iteration 243, loss = 0.01031880\n",
            "Iteration 244, loss = 0.01023879\n",
            "Iteration 245, loss = 0.01024026\n",
            "Iteration 246, loss = 0.01016470\n",
            "Iteration 247, loss = 0.01008771\n",
            "Iteration 248, loss = 0.01008599\n",
            "Iteration 249, loss = 0.00993089\n",
            "Iteration 250, loss = 0.00994076\n",
            "Iteration 251, loss = 0.00998260\n",
            "Iteration 252, loss = 0.00985173\n",
            "Iteration 253, loss = 0.00989452\n",
            "Iteration 254, loss = 0.00972596\n",
            "Iteration 255, loss = 0.00966420\n",
            "Iteration 256, loss = 0.00961646\n",
            "Iteration 257, loss = 0.00966050\n",
            "Iteration 258, loss = 0.00952117\n",
            "Iteration 259, loss = 0.00948176\n",
            "Iteration 260, loss = 0.00948327\n",
            "Iteration 261, loss = 0.00942306\n",
            "Iteration 262, loss = 0.00937332\n",
            "Iteration 263, loss = 0.00927884\n",
            "Iteration 264, loss = 0.00925058\n",
            "Iteration 265, loss = 0.00927419\n",
            "Iteration 266, loss = 0.00915276\n",
            "Iteration 267, loss = 0.00910876\n",
            "Iteration 268, loss = 0.00912965\n",
            "Iteration 269, loss = 0.00898093\n",
            "Iteration 270, loss = 0.00899806\n",
            "Iteration 271, loss = 0.00902156\n",
            "Iteration 272, loss = 0.00890591\n",
            "Iteration 273, loss = 0.00890986\n",
            "Iteration 274, loss = 0.00890797\n",
            "Iteration 275, loss = 0.00886421\n",
            "Iteration 276, loss = 0.00874950\n",
            "Iteration 277, loss = 0.00883933\n",
            "Iteration 278, loss = 0.00858825\n",
            "Iteration 279, loss = 0.00879760\n",
            "Iteration 280, loss = 0.00857026\n",
            "Iteration 281, loss = 0.00880082\n",
            "Iteration 282, loss = 0.00853911\n",
            "Iteration 283, loss = 0.00871982\n",
            "Iteration 284, loss = 0.00843511\n",
            "Iteration 285, loss = 0.00835580\n",
            "Iteration 286, loss = 0.00830321\n",
            "Iteration 287, loss = 0.00828468\n",
            "Iteration 288, loss = 0.00823140\n",
            "Iteration 289, loss = 0.00821815\n",
            "Iteration 290, loss = 0.00816587\n",
            "Iteration 291, loss = 0.00812713\n",
            "Iteration 292, loss = 0.00812298\n",
            "Iteration 293, loss = 0.00801681\n",
            "Iteration 294, loss = 0.00803763\n",
            "Iteration 295, loss = 0.00811181\n",
            "Iteration 296, loss = 0.00797493\n",
            "Iteration 297, loss = 0.00793449\n",
            "Iteration 298, loss = 0.00789981\n",
            "Iteration 299, loss = 0.00776161\n",
            "Iteration 300, loss = 0.00780790\n",
            "Iteration 301, loss = 0.00773265\n",
            "Iteration 302, loss = 0.00771361\n",
            "Iteration 303, loss = 0.00763361\n",
            "Iteration 304, loss = 0.00776579\n",
            "Iteration 305, loss = 0.00760315\n",
            "Iteration 306, loss = 0.00779161\n",
            "Iteration 307, loss = 0.00762224\n",
            "Iteration 308, loss = 0.00755042\n",
            "Iteration 309, loss = 0.00750165\n",
            "Iteration 310, loss = 0.00744754\n",
            "Iteration 311, loss = 0.00746968\n",
            "Iteration 312, loss = 0.00731514\n",
            "Iteration 313, loss = 0.00734323\n",
            "Iteration 314, loss = 0.00741411\n",
            "Iteration 315, loss = 0.00729349\n",
            "Iteration 316, loss = 0.00722662\n",
            "Iteration 317, loss = 0.00719751\n",
            "Iteration 318, loss = 0.00720530\n",
            "Iteration 319, loss = 0.00716578\n",
            "Iteration 320, loss = 0.00712015\n",
            "Iteration 321, loss = 0.00711009\n",
            "Iteration 322, loss = 0.00706134\n",
            "Iteration 323, loss = 0.00698609\n",
            "Iteration 324, loss = 0.00704097\n",
            "Iteration 325, loss = 0.00699020\n",
            "Iteration 326, loss = 0.00687635\n",
            "Iteration 327, loss = 0.00690249\n",
            "Iteration 328, loss = 0.00685850\n",
            "Iteration 329, loss = 0.00684781\n",
            "Iteration 330, loss = 0.00692075\n",
            "Iteration 331, loss = 0.00679746\n",
            "Iteration 332, loss = 0.00684486\n",
            "Iteration 333, loss = 0.00691667\n",
            "Iteration 334, loss = 0.00670543\n",
            "Iteration 335, loss = 0.00677871\n",
            "Iteration 336, loss = 0.00662193\n",
            "Iteration 337, loss = 0.00663645\n",
            "Iteration 338, loss = 0.00657270\n",
            "Iteration 339, loss = 0.00654961\n",
            "Iteration 340, loss = 0.00655181\n",
            "Iteration 341, loss = 0.00652470\n",
            "Iteration 342, loss = 0.00651471\n",
            "Iteration 343, loss = 0.00639407\n",
            "Iteration 344, loss = 0.00644145\n",
            "Iteration 345, loss = 0.00643663\n",
            "Iteration 346, loss = 0.00644933\n",
            "Iteration 347, loss = 0.00635890\n",
            "Iteration 348, loss = 0.00634690\n",
            "Iteration 349, loss = 0.00629456\n",
            "Iteration 350, loss = 0.00633370\n",
            "Iteration 351, loss = 0.00631049\n",
            "Iteration 352, loss = 0.00624662\n",
            "Iteration 353, loss = 0.00631912\n",
            "Iteration 354, loss = 0.00626503\n",
            "Iteration 355, loss = 0.00614511\n",
            "Iteration 356, loss = 0.00611462\n",
            "Iteration 357, loss = 0.00620019\n",
            "Iteration 358, loss = 0.00611475\n",
            "Iteration 359, loss = 0.00607991\n",
            "Iteration 360, loss = 0.00599930\n",
            "Iteration 361, loss = 0.00596829\n",
            "Iteration 362, loss = 0.00611293\n",
            "Iteration 363, loss = 0.00597305\n",
            "Iteration 364, loss = 0.00595986\n",
            "Iteration 365, loss = 0.00588183\n",
            "Iteration 366, loss = 0.00584288\n",
            "Iteration 367, loss = 0.00590326\n",
            "Iteration 368, loss = 0.00582384\n",
            "Iteration 369, loss = 0.00588549\n",
            "Iteration 370, loss = 0.00581804\n",
            "Iteration 371, loss = 0.00574938\n",
            "Iteration 372, loss = 0.00565376\n",
            "Iteration 373, loss = 0.00570679\n",
            "Iteration 374, loss = 0.00574801\n",
            "Iteration 375, loss = 0.00571390\n",
            "Iteration 376, loss = 0.00560008\n",
            "Iteration 377, loss = 0.00561051\n",
            "Iteration 378, loss = 0.00558799\n",
            "Iteration 379, loss = 0.00559729\n",
            "Iteration 380, loss = 0.00564254\n",
            "Iteration 381, loss = 0.00563034\n",
            "Iteration 382, loss = 0.00552764\n",
            "Iteration 383, loss = 0.00547031\n",
            "Iteration 384, loss = 0.00545956\n",
            "Iteration 385, loss = 0.00541614\n",
            "Iteration 386, loss = 0.00548735\n",
            "Iteration 387, loss = 0.00536773\n",
            "Iteration 388, loss = 0.00536053\n",
            "Iteration 389, loss = 0.00532131\n",
            "Iteration 390, loss = 0.00535846\n",
            "Iteration 391, loss = 0.00536045\n",
            "Iteration 392, loss = 0.00531980\n",
            "Iteration 393, loss = 0.00520550\n",
            "Iteration 394, loss = 0.00534411\n",
            "Iteration 395, loss = 0.00523441\n",
            "Iteration 396, loss = 0.00519355\n",
            "Iteration 397, loss = 0.00520971\n",
            "Iteration 398, loss = 0.00508752\n",
            "Iteration 399, loss = 0.00516852\n",
            "Iteration 400, loss = 0.00510433\n",
            "Iteration 401, loss = 0.00517805\n",
            "Iteration 402, loss = 0.00503997\n",
            "Iteration 403, loss = 0.00533245\n",
            "Iteration 404, loss = 0.00518450\n",
            "Iteration 405, loss = 0.00518804\n",
            "Iteration 406, loss = 0.00497261\n",
            "Iteration 407, loss = 0.00494036\n",
            "Iteration 408, loss = 0.00500289\n",
            "Iteration 409, loss = 0.00506562\n",
            "Iteration 410, loss = 0.00490474\n",
            "Iteration 411, loss = 0.00508850\n",
            "Iteration 412, loss = 0.00508706\n",
            "Iteration 413, loss = 0.00493256\n",
            "Iteration 414, loss = 0.00484851\n",
            "Iteration 415, loss = 0.00485467\n",
            "Iteration 416, loss = 0.00485257\n",
            "Iteration 417, loss = 0.00478119\n",
            "Iteration 418, loss = 0.00476727\n",
            "Iteration 419, loss = 0.00487018\n",
            "Iteration 420, loss = 0.00475894\n",
            "Iteration 421, loss = 0.00485268\n",
            "Iteration 422, loss = 0.00470266\n",
            "Iteration 423, loss = 0.00467439\n",
            "Iteration 424, loss = 0.00467128\n",
            "Iteration 425, loss = 0.00473350\n",
            "Iteration 426, loss = 0.00468845\n",
            "Iteration 427, loss = 0.00479199\n",
            "Iteration 428, loss = 0.00452851\n",
            "Iteration 429, loss = 0.00478087\n",
            "Iteration 430, loss = 0.00461456\n",
            "Iteration 431, loss = 0.00451315\n",
            "Iteration 432, loss = 0.00445253\n",
            "Iteration 433, loss = 0.00447296\n",
            "Iteration 434, loss = 0.00457771\n",
            "Iteration 435, loss = 0.00444551\n",
            "Iteration 436, loss = 0.00444533\n",
            "Iteration 437, loss = 0.00448279\n",
            "Iteration 438, loss = 0.00438638\n",
            "Iteration 439, loss = 0.00441603\n",
            "Iteration 440, loss = 0.00441228\n",
            "Iteration 441, loss = 0.00445700\n",
            "Iteration 442, loss = 0.00436413\n",
            "Iteration 443, loss = 0.00438735\n",
            "Iteration 444, loss = 0.00425898\n",
            "Iteration 445, loss = 0.00444932\n",
            "Iteration 446, loss = 0.00438476\n",
            "Iteration 447, loss = 0.00422966\n",
            "Iteration 448, loss = 0.00429435\n",
            "Iteration 449, loss = 0.00433354\n",
            "Iteration 450, loss = 0.00451758\n",
            "Iteration 451, loss = 0.00418392\n",
            "Iteration 452, loss = 0.00422306\n",
            "Iteration 453, loss = 0.00416887\n",
            "Iteration 454, loss = 0.00409055\n",
            "Iteration 455, loss = 0.00409875\n",
            "Iteration 456, loss = 0.00427530\n",
            "Iteration 457, loss = 0.00416366\n",
            "Iteration 458, loss = 0.00410198\n",
            "Iteration 459, loss = 0.00403206\n",
            "Iteration 460, loss = 0.00401753\n",
            "Iteration 461, loss = 0.00405213\n",
            "Iteration 462, loss = 0.00404051\n",
            "Iteration 463, loss = 0.00408668\n",
            "Iteration 464, loss = 0.00404341\n",
            "Iteration 465, loss = 0.00406415\n",
            "Iteration 466, loss = 0.00399102\n",
            "Iteration 467, loss = 0.00390870\n",
            "Iteration 468, loss = 0.00391168\n",
            "Iteration 469, loss = 0.00391259\n",
            "Iteration 470, loss = 0.00399910\n",
            "Iteration 471, loss = 0.00391507\n",
            "Iteration 472, loss = 0.00387344\n",
            "Iteration 473, loss = 0.00382631\n",
            "Iteration 474, loss = 0.00385913\n",
            "Iteration 475, loss = 0.00385152\n",
            "Iteration 476, loss = 0.00380252\n",
            "Iteration 477, loss = 0.00383967\n",
            "Iteration 478, loss = 0.00375521\n",
            "Iteration 479, loss = 0.00379654\n",
            "Iteration 480, loss = 0.00379104\n",
            "Iteration 481, loss = 0.00378063\n",
            "Iteration 482, loss = 0.00383462\n",
            "Iteration 483, loss = 0.00383179\n",
            "Iteration 484, loss = 0.00375138\n",
            "Iteration 485, loss = 0.00368026\n",
            "Iteration 486, loss = 0.00370297\n",
            "Iteration 487, loss = 0.00363420\n",
            "Iteration 488, loss = 0.00376505\n",
            "Iteration 489, loss = 0.00365132\n",
            "Iteration 490, loss = 0.00363777\n",
            "Iteration 491, loss = 0.00364778\n",
            "Iteration 492, loss = 0.00355992\n",
            "Iteration 493, loss = 0.00358224\n",
            "Iteration 494, loss = 0.00354859\n",
            "Iteration 495, loss = 0.00354144\n",
            "Iteration 496, loss = 0.00353673\n",
            "Iteration 497, loss = 0.00357390\n",
            "Iteration 498, loss = 0.00352343\n",
            "Iteration 499, loss = 0.00357037\n",
            "Iteration 500, loss = 0.00353308\n",
            "Iteration 501, loss = 0.00353400\n",
            "Iteration 502, loss = 0.00347854\n",
            "Iteration 503, loss = 0.00337420\n",
            "Iteration 504, loss = 0.00350393\n",
            "Iteration 505, loss = 0.00340373\n",
            "Iteration 506, loss = 0.00340594\n",
            "Iteration 507, loss = 0.00345054\n",
            "Iteration 508, loss = 0.00334596\n",
            "Iteration 509, loss = 0.00341568\n",
            "Iteration 510, loss = 0.00335732\n",
            "Iteration 511, loss = 0.00338157\n",
            "Iteration 512, loss = 0.00334444\n",
            "Iteration 513, loss = 0.00334325\n",
            "Iteration 514, loss = 0.00328604\n",
            "Iteration 515, loss = 0.00333607\n",
            "Iteration 516, loss = 0.00327607\n",
            "Iteration 517, loss = 0.00327704\n",
            "Iteration 518, loss = 0.00331456\n",
            "Iteration 519, loss = 0.00325705\n",
            "Iteration 520, loss = 0.00329607\n",
            "Iteration 521, loss = 0.00338251\n",
            "Iteration 522, loss = 0.00331171\n",
            "Iteration 523, loss = 0.00317954\n",
            "Iteration 524, loss = 0.00318591\n",
            "Iteration 525, loss = 0.00320805\n",
            "Iteration 526, loss = 0.00314866\n",
            "Iteration 527, loss = 0.00319509\n",
            "Iteration 528, loss = 0.00320406\n",
            "Iteration 529, loss = 0.00318008\n",
            "Iteration 530, loss = 0.00317056\n",
            "Iteration 531, loss = 0.00307532\n",
            "Iteration 532, loss = 0.00317435\n",
            "Iteration 533, loss = 0.00302833\n",
            "Iteration 534, loss = 0.00316438\n",
            "Iteration 535, loss = 0.00308668\n",
            "Iteration 536, loss = 0.00309083\n",
            "Iteration 537, loss = 0.00309257\n",
            "Iteration 538, loss = 0.00319780\n",
            "Iteration 539, loss = 0.00300166\n",
            "Iteration 540, loss = 0.00304416\n",
            "Iteration 541, loss = 0.00300229\n",
            "Iteration 542, loss = 0.00305763\n",
            "Iteration 543, loss = 0.00304228\n",
            "Iteration 544, loss = 0.00295302\n",
            "Iteration 545, loss = 0.00292833\n",
            "Iteration 546, loss = 0.00291994\n",
            "Iteration 547, loss = 0.00292301\n",
            "Iteration 548, loss = 0.00291439\n",
            "Iteration 549, loss = 0.00299819\n",
            "Iteration 550, loss = 0.00284629\n",
            "Iteration 551, loss = 0.00290715\n",
            "Iteration 552, loss = 0.00290420\n",
            "Iteration 553, loss = 0.00293136\n",
            "Iteration 554, loss = 0.00282865\n",
            "Iteration 555, loss = 0.00285340\n",
            "Iteration 556, loss = 0.00285374\n",
            "Iteration 557, loss = 0.00289412\n",
            "Iteration 558, loss = 0.00293834\n",
            "Iteration 559, loss = 0.00281718\n",
            "Iteration 560, loss = 0.00281867\n",
            "Iteration 561, loss = 0.00282053\n",
            "Iteration 562, loss = 0.00289240\n",
            "Iteration 563, loss = 0.00293365\n",
            "Iteration 564, loss = 0.00286420\n",
            "Iteration 565, loss = 0.00276917\n",
            "Iteration 566, loss = 0.00282326\n",
            "Iteration 567, loss = 0.00271066\n",
            "Iteration 568, loss = 0.00277106\n",
            "Iteration 569, loss = 0.00278560\n",
            "Iteration 570, loss = 0.00280895\n",
            "Iteration 571, loss = 0.00277696\n",
            "Iteration 572, loss = 0.00268886\n",
            "Iteration 573, loss = 0.00272377\n",
            "Iteration 574, loss = 0.00274478\n",
            "Iteration 575, loss = 0.00263296\n",
            "Iteration 576, loss = 0.00268591\n",
            "Iteration 577, loss = 0.00277873\n",
            "Iteration 578, loss = 0.00269806\n",
            "Iteration 579, loss = 0.00258626\n",
            "Iteration 580, loss = 0.00261225\n",
            "Iteration 581, loss = 0.00267659\n",
            "Iteration 582, loss = 0.00277693\n",
            "Iteration 583, loss = 0.00257188\n",
            "Iteration 584, loss = 0.00262725\n",
            "Iteration 585, loss = 0.00260788\n",
            "Iteration 586, loss = 0.00269750\n",
            "Iteration 587, loss = 0.00257507\n",
            "Iteration 588, loss = 0.00258505\n",
            "Iteration 589, loss = 0.00256644\n",
            "Iteration 590, loss = 0.00256612\n",
            "Iteration 591, loss = 0.00259443\n",
            "Iteration 592, loss = 0.00250203\n",
            "Iteration 593, loss = 0.00255389\n",
            "Iteration 594, loss = 0.00248963\n",
            "Iteration 595, loss = 0.00254717\n",
            "Iteration 596, loss = 0.00263493\n",
            "Iteration 597, loss = 0.00250001\n",
            "Iteration 598, loss = 0.00242587\n",
            "Iteration 599, loss = 0.00251811\n",
            "Iteration 600, loss = 0.00245208\n",
            "Iteration 601, loss = 0.00241531\n",
            "Iteration 602, loss = 0.00241379\n",
            "Iteration 603, loss = 0.00240095\n",
            "Iteration 604, loss = 0.00241413\n",
            "Iteration 605, loss = 0.00238812\n",
            "Iteration 606, loss = 0.00240259\n",
            "Iteration 607, loss = 0.00242282\n",
            "Iteration 608, loss = 0.00239929\n",
            "Iteration 609, loss = 0.00239572\n",
            "Iteration 610, loss = 0.00235255\n",
            "Iteration 611, loss = 0.00236479\n",
            "Iteration 612, loss = 0.00233850\n",
            "Iteration 613, loss = 0.00232980\n",
            "Iteration 614, loss = 0.00230908\n",
            "Iteration 615, loss = 0.00238734\n",
            "Iteration 616, loss = 0.00228527\n",
            "Iteration 617, loss = 0.00237469\n",
            "Iteration 618, loss = 0.00230579\n",
            "Iteration 619, loss = 0.00233597\n",
            "Iteration 620, loss = 0.00233202\n",
            "Iteration 621, loss = 0.00238632\n",
            "Iteration 622, loss = 0.00228517\n",
            "Iteration 623, loss = 0.00228481\n",
            "Iteration 624, loss = 0.00227598\n",
            "Iteration 625, loss = 0.00237467\n",
            "Iteration 626, loss = 0.00224359\n",
            "Iteration 627, loss = 0.00229403\n",
            "Iteration 628, loss = 0.00222601\n",
            "Iteration 629, loss = 0.00225191\n",
            "Iteration 630, loss = 0.00220744\n",
            "Iteration 631, loss = 0.00225767\n",
            "Iteration 632, loss = 0.00229545\n",
            "Iteration 633, loss = 0.00221979\n",
            "Iteration 634, loss = 0.00222058\n",
            "Iteration 635, loss = 0.00214025\n",
            "Iteration 636, loss = 0.00214219\n",
            "Iteration 637, loss = 0.00227631\n",
            "Iteration 638, loss = 0.00231465\n",
            "Iteration 639, loss = 0.00225847\n",
            "Iteration 640, loss = 0.00219381\n",
            "Iteration 641, loss = 0.00210523\n",
            "Iteration 642, loss = 0.00213102\n",
            "Iteration 643, loss = 0.00208961\n",
            "Iteration 644, loss = 0.00207664\n",
            "Iteration 645, loss = 0.00211626\n",
            "Iteration 646, loss = 0.00206271\n",
            "Iteration 647, loss = 0.00209466\n",
            "Iteration 648, loss = 0.00210346\n",
            "Iteration 649, loss = 0.00205022\n",
            "Iteration 650, loss = 0.00208952\n",
            "Iteration 651, loss = 0.00201851\n",
            "Iteration 652, loss = 0.00210533\n",
            "Iteration 653, loss = 0.00210497\n",
            "Iteration 654, loss = 0.00208530\n",
            "Iteration 655, loss = 0.00204610\n",
            "Iteration 656, loss = 0.00199292\n",
            "Iteration 657, loss = 0.00200663\n",
            "Iteration 658, loss = 0.00200686\n",
            "Iteration 659, loss = 0.00198574\n",
            "Iteration 660, loss = 0.00199178\n",
            "Iteration 661, loss = 0.00202362\n",
            "Iteration 662, loss = 0.00201502\n",
            "Iteration 663, loss = 0.00200439\n",
            "Iteration 664, loss = 0.00197473\n",
            "Iteration 665, loss = 0.00199560\n",
            "Iteration 666, loss = 0.00194752\n",
            "Iteration 667, loss = 0.00194305\n",
            "Iteration 668, loss = 0.00191696\n",
            "Iteration 669, loss = 0.00193916\n",
            "Iteration 670, loss = 0.00193115\n",
            "Iteration 671, loss = 0.00201192\n",
            "Iteration 672, loss = 0.00191226\n",
            "Iteration 673, loss = 0.00209866\n",
            "Iteration 674, loss = 0.00189452\n",
            "Iteration 675, loss = 0.00196694\n",
            "Iteration 676, loss = 0.00190161\n",
            "Iteration 677, loss = 0.00190301\n",
            "Iteration 678, loss = 0.00190745\n",
            "Iteration 679, loss = 0.00183353\n",
            "Iteration 680, loss = 0.00193214\n",
            "Iteration 681, loss = 0.00183138\n",
            "Iteration 682, loss = 0.00198150\n",
            "Iteration 683, loss = 0.00198880\n",
            "Iteration 684, loss = 0.00183630\n",
            "Iteration 685, loss = 0.00183995\n",
            "Iteration 686, loss = 0.00183866\n",
            "Iteration 687, loss = 0.00182082\n",
            "Iteration 688, loss = 0.00177923\n",
            "Iteration 689, loss = 0.00179802\n",
            "Iteration 690, loss = 0.00179467\n",
            "Iteration 691, loss = 0.00179935\n",
            "Iteration 692, loss = 0.00182994\n",
            "Iteration 693, loss = 0.00182128\n",
            "Iteration 694, loss = 0.00187061\n",
            "Iteration 695, loss = 0.00180927\n",
            "Iteration 696, loss = 0.00199875\n",
            "Iteration 697, loss = 0.00180380\n",
            "Iteration 698, loss = 0.00187968\n",
            "Iteration 699, loss = 0.00199574\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)"
            ]
          },
          "execution_count": 303,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3 -> 100 -> 100 -> 1\n",
        "# 3 -> 2 -> 2 -> 1\n",
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
        "                                   solver = 'adam', activation = 'relu',\n",
        "                                   hidden_layer_sizes = (20,20))\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bmn64K7vqzx1",
        "outputId": "f2470659-baf2-4888-a344-2738f35894fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "execution_count": 304,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
        "previsoes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzFRj9riq9gc",
        "outputId": "7aec0086-4ed8-4b3f-c2f8-78b640691b0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "execution_count": 305,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_credit_teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vRZgbSerBNW",
        "outputId": "27263d4c-5ff6-44f1-eb1c-21b7c57ac893"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "execution_count": 306,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(y_credit_teste, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "XwtALw-Mronz",
        "outputId": "cdf4d1b0-486b-4d3e-ab90-817aee4269e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "execution_count": 307,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(X_credit_teste, y_credit_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWj1N8ANsYrI",
        "outputId": "e50c8c22-a459-4886-efb1-10f98ebd09ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       0.98      0.98      0.98        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_credit_teste, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeiHfvihtMZ"
      },
      "source": [
        "## Base census - 81.53%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV0uo4LFs-GU"
      },
      "outputs": [],
      "source": [
        "with open('census.pkl', 'rb') as f:\n",
        "  X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8Xnk0tztAWw",
        "outputId": "e88866c0-3feb-490d-d729-43c932fbe16f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((27676, 108), (27676,))"
            ]
          },
          "execution_count": 310,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_census_treinamento.shape, y_census_treinamento.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWPc6R6NtCyf",
        "outputId": "d87f5cef-aeb2-41c8-8ab8-184069719b74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4885, 108), (4885,))"
            ]
          },
          "execution_count": 311,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_census_teste.shape, y_census_teste.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIhQjIv-thWw",
        "outputId": "cb48de09-31ff-4123-f489-78bb34f8cd0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "54.5"
            ]
          },
          "execution_count": 312,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(108 + 1) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewuPX3oNtSgX",
        "outputId": "8a26988e-36eb-4821-eb9e-1cac5ac70b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.37814472\n",
            "Iteration 2, loss = 0.32606784\n",
            "Iteration 3, loss = 0.31542839\n",
            "Iteration 4, loss = 0.30773550\n",
            "Iteration 5, loss = 0.30301734\n",
            "Iteration 6, loss = 0.29825885\n",
            "Iteration 7, loss = 0.29621419\n",
            "Iteration 8, loss = 0.29253435\n",
            "Iteration 9, loss = 0.29002011\n",
            "Iteration 10, loss = 0.28811358\n",
            "Iteration 11, loss = 0.28556142\n",
            "Iteration 12, loss = 0.28417865\n",
            "Iteration 13, loss = 0.28169615\n",
            "Iteration 14, loss = 0.28013194\n",
            "Iteration 15, loss = 0.27794341\n",
            "Iteration 16, loss = 0.27701328\n",
            "Iteration 17, loss = 0.27532934\n",
            "Iteration 18, loss = 0.27319446\n",
            "Iteration 19, loss = 0.27205939\n",
            "Iteration 20, loss = 0.27089301\n",
            "Iteration 21, loss = 0.26886973\n",
            "Iteration 22, loss = 0.26795546\n",
            "Iteration 23, loss = 0.26620646\n",
            "Iteration 24, loss = 0.26483678\n",
            "Iteration 25, loss = 0.26339567\n",
            "Iteration 26, loss = 0.26248125\n",
            "Iteration 27, loss = 0.26090540\n",
            "Iteration 28, loss = 0.25973849\n",
            "Iteration 29, loss = 0.25762769\n",
            "Iteration 30, loss = 0.25749517\n",
            "Iteration 31, loss = 0.25525681\n",
            "Iteration 32, loss = 0.25455152\n",
            "Iteration 33, loss = 0.25294508\n",
            "Iteration 34, loss = 0.25197151\n",
            "Iteration 35, loss = 0.25016016\n",
            "Iteration 36, loss = 0.24928143\n",
            "Iteration 37, loss = 0.24806073\n",
            "Iteration 38, loss = 0.24722011\n",
            "Iteration 39, loss = 0.24629508\n",
            "Iteration 40, loss = 0.24449316\n",
            "Iteration 41, loss = 0.24428111\n",
            "Iteration 42, loss = 0.24331133\n",
            "Iteration 43, loss = 0.24059655\n",
            "Iteration 44, loss = 0.24021237\n",
            "Iteration 45, loss = 0.23922481\n",
            "Iteration 46, loss = 0.23806214\n",
            "Iteration 47, loss = 0.23662461\n",
            "Iteration 48, loss = 0.23605539\n",
            "Iteration 49, loss = 0.23589046\n",
            "Iteration 50, loss = 0.23408884\n",
            "Iteration 51, loss = 0.23269212\n",
            "Iteration 52, loss = 0.23247088\n",
            "Iteration 53, loss = 0.23089777\n",
            "Iteration 54, loss = 0.22963129\n",
            "Iteration 55, loss = 0.22930643\n",
            "Iteration 56, loss = 0.22872375\n",
            "Iteration 57, loss = 0.22828348\n",
            "Iteration 58, loss = 0.22686101\n",
            "Iteration 59, loss = 0.22554335\n",
            "Iteration 60, loss = 0.22450276\n",
            "Iteration 61, loss = 0.22470990\n",
            "Iteration 62, loss = 0.22446120\n",
            "Iteration 63, loss = 0.22386927\n",
            "Iteration 64, loss = 0.22269586\n",
            "Iteration 65, loss = 0.22215225\n",
            "Iteration 66, loss = 0.22094631\n",
            "Iteration 67, loss = 0.22084768\n",
            "Iteration 68, loss = 0.21976626\n",
            "Iteration 69, loss = 0.21902751\n",
            "Iteration 70, loss = 0.21767754\n",
            "Iteration 71, loss = 0.21683420\n",
            "Iteration 72, loss = 0.21715180\n",
            "Iteration 73, loss = 0.21719107\n",
            "Iteration 74, loss = 0.21544483\n",
            "Iteration 75, loss = 0.21372707\n",
            "Iteration 76, loss = 0.21384306\n",
            "Iteration 77, loss = 0.21329713\n",
            "Iteration 78, loss = 0.21238882\n",
            "Iteration 79, loss = 0.21180975\n",
            "Iteration 80, loss = 0.21223378\n",
            "Iteration 81, loss = 0.21089433\n",
            "Iteration 82, loss = 0.21001806\n",
            "Iteration 83, loss = 0.20899917\n",
            "Iteration 84, loss = 0.20873350\n",
            "Iteration 85, loss = 0.20779110\n",
            "Iteration 86, loss = 0.20788068\n",
            "Iteration 87, loss = 0.20702079\n",
            "Iteration 88, loss = 0.20641561\n",
            "Iteration 89, loss = 0.20690637\n",
            "Iteration 90, loss = 0.20739226\n",
            "Iteration 91, loss = 0.20499416\n",
            "Iteration 92, loss = 0.20449175\n",
            "Iteration 93, loss = 0.20328237\n",
            "Iteration 94, loss = 0.20353209\n",
            "Iteration 95, loss = 0.20305050\n",
            "Iteration 96, loss = 0.20308506\n",
            "Iteration 97, loss = 0.20276210\n",
            "Iteration 98, loss = 0.20157273\n",
            "Iteration 99, loss = 0.20131256\n",
            "Iteration 100, loss = 0.20152575\n",
            "Iteration 101, loss = 0.20026337\n",
            "Iteration 102, loss = 0.19883616\n",
            "Iteration 103, loss = 0.19932874\n",
            "Iteration 104, loss = 0.19884369\n",
            "Iteration 105, loss = 0.20096367\n",
            "Iteration 106, loss = 0.19860635\n",
            "Iteration 107, loss = 0.19777305\n",
            "Iteration 108, loss = 0.19618986\n",
            "Iteration 109, loss = 0.19709577\n",
            "Iteration 110, loss = 0.19645928\n",
            "Iteration 111, loss = 0.19578464\n",
            "Iteration 112, loss = 0.19525620\n",
            "Iteration 113, loss = 0.19630279\n",
            "Iteration 114, loss = 0.19439250\n",
            "Iteration 115, loss = 0.19343208\n",
            "Iteration 116, loss = 0.19328928\n",
            "Iteration 117, loss = 0.19424806\n",
            "Iteration 118, loss = 0.19324647\n",
            "Iteration 119, loss = 0.19375485\n",
            "Iteration 120, loss = 0.19131565\n",
            "Iteration 121, loss = 0.19173245\n",
            "Iteration 122, loss = 0.19172234\n",
            "Iteration 123, loss = 0.19156743\n",
            "Iteration 124, loss = 0.19115979\n",
            "Iteration 125, loss = 0.19028502\n",
            "Iteration 126, loss = 0.18999258\n",
            "Iteration 127, loss = 0.18917595\n",
            "Iteration 128, loss = 0.18905066\n",
            "Iteration 129, loss = 0.18889811\n",
            "Iteration 130, loss = 0.18878936\n",
            "Iteration 131, loss = 0.18852582\n",
            "Iteration 132, loss = 0.18806828\n",
            "Iteration 133, loss = 0.18921787\n",
            "Iteration 134, loss = 0.18561324\n",
            "Iteration 135, loss = 0.18664525\n",
            "Iteration 136, loss = 0.18567250\n",
            "Iteration 137, loss = 0.18759268\n",
            "Iteration 138, loss = 0.18493596\n",
            "Iteration 139, loss = 0.18493575\n",
            "Iteration 140, loss = 0.18543858\n",
            "Iteration 141, loss = 0.18469926\n",
            "Iteration 142, loss = 0.18425466\n",
            "Iteration 143, loss = 0.18420854\n",
            "Iteration 144, loss = 0.18265062\n",
            "Iteration 145, loss = 0.18320435\n",
            "Iteration 146, loss = 0.18349997\n",
            "Iteration 147, loss = 0.18222547\n",
            "Iteration 148, loss = 0.18282123\n",
            "Iteration 149, loss = 0.18192513\n",
            "Iteration 150, loss = 0.18146949\n",
            "Iteration 151, loss = 0.18227997\n",
            "Iteration 152, loss = 0.18086524\n",
            "Iteration 153, loss = 0.17967149\n",
            "Iteration 154, loss = 0.18170453\n",
            "Iteration 155, loss = 0.18150422\n",
            "Iteration 156, loss = 0.17942736\n",
            "Iteration 157, loss = 0.17992668\n",
            "Iteration 158, loss = 0.18021480\n",
            "Iteration 159, loss = 0.17875855\n",
            "Iteration 160, loss = 0.17829459\n",
            "Iteration 161, loss = 0.17996110\n",
            "Iteration 162, loss = 0.17931120\n",
            "Iteration 163, loss = 0.17845488\n",
            "Iteration 164, loss = 0.17758649\n",
            "Iteration 165, loss = 0.17741327\n",
            "Iteration 166, loss = 0.17827918\n",
            "Iteration 167, loss = 0.17761018\n",
            "Iteration 168, loss = 0.17790177\n",
            "Iteration 169, loss = 0.17838355\n",
            "Iteration 170, loss = 0.17581180\n",
            "Iteration 171, loss = 0.17663571\n",
            "Iteration 172, loss = 0.17544784\n",
            "Iteration 173, loss = 0.17582447\n",
            "Iteration 174, loss = 0.17572633\n",
            "Iteration 175, loss = 0.17532129\n",
            "Iteration 176, loss = 0.17443909\n",
            "Iteration 177, loss = 0.17615433\n",
            "Iteration 178, loss = 0.17373762\n",
            "Iteration 179, loss = 0.17403030\n",
            "Iteration 180, loss = 0.17372254\n",
            "Iteration 181, loss = 0.17478788\n",
            "Iteration 182, loss = 0.17387460\n",
            "Iteration 183, loss = 0.17221353\n",
            "Iteration 184, loss = 0.17331403\n",
            "Iteration 185, loss = 0.17155914\n",
            "Iteration 186, loss = 0.17087033\n",
            "Iteration 187, loss = 0.17198799\n",
            "Iteration 188, loss = 0.17133716\n",
            "Iteration 189, loss = 0.17161602\n",
            "Iteration 190, loss = 0.17258956\n",
            "Iteration 191, loss = 0.17025517\n",
            "Iteration 192, loss = 0.17068423\n",
            "Iteration 193, loss = 0.16982649\n",
            "Iteration 194, loss = 0.16940004\n",
            "Iteration 195, loss = 0.17001897\n",
            "Iteration 196, loss = 0.16879127\n",
            "Iteration 197, loss = 0.16883451\n",
            "Iteration 198, loss = 0.16955226\n",
            "Iteration 199, loss = 0.17035318\n",
            "Iteration 200, loss = 0.16965458\n",
            "Iteration 201, loss = 0.16650446\n",
            "Iteration 202, loss = 0.16960007\n",
            "Iteration 203, loss = 0.16929559\n",
            "Iteration 204, loss = 0.16767916\n",
            "Iteration 205, loss = 0.16811462\n",
            "Iteration 206, loss = 0.16839927\n",
            "Iteration 207, loss = 0.16721278\n",
            "Iteration 208, loss = 0.16706193\n",
            "Iteration 209, loss = 0.16633506\n",
            "Iteration 210, loss = 0.16813713\n",
            "Iteration 211, loss = 0.16746376\n",
            "Iteration 212, loss = 0.16629053\n",
            "Iteration 213, loss = 0.16623464\n",
            "Iteration 214, loss = 0.16762805\n",
            "Iteration 215, loss = 0.16502597\n",
            "Iteration 216, loss = 0.16473616\n",
            "Iteration 217, loss = 0.16523822\n",
            "Iteration 218, loss = 0.16519985\n",
            "Iteration 219, loss = 0.16526773\n",
            "Iteration 220, loss = 0.16330209\n",
            "Iteration 221, loss = 0.16287546\n",
            "Iteration 222, loss = 0.16412050\n",
            "Iteration 223, loss = 0.16316161\n",
            "Iteration 224, loss = 0.16496652\n",
            "Iteration 225, loss = 0.16497454\n",
            "Iteration 226, loss = 0.16518897\n",
            "Iteration 227, loss = 0.16452977\n",
            "Iteration 228, loss = 0.16190197\n",
            "Iteration 229, loss = 0.16267847\n",
            "Iteration 230, loss = 0.16342061\n",
            "Iteration 231, loss = 0.16553689\n",
            "Iteration 232, loss = 0.16190076\n",
            "Iteration 233, loss = 0.16289724\n",
            "Iteration 234, loss = 0.16444443\n",
            "Iteration 235, loss = 0.16183549\n",
            "Iteration 236, loss = 0.16048169\n",
            "Iteration 237, loss = 0.16037929\n",
            "Iteration 238, loss = 0.15975931\n",
            "Iteration 239, loss = 0.16232836\n",
            "Iteration 240, loss = 0.16388108\n",
            "Iteration 241, loss = 0.16167437\n",
            "Iteration 242, loss = 0.16034694\n",
            "Iteration 243, loss = 0.16121861\n",
            "Iteration 244, loss = 0.16090278\n",
            "Iteration 245, loss = 0.15979273\n",
            "Iteration 246, loss = 0.16116175\n",
            "Iteration 247, loss = 0.16107575\n",
            "Iteration 248, loss = 0.16114082\n",
            "Iteration 249, loss = 0.15807728\n",
            "Iteration 250, loss = 0.15836501\n",
            "Iteration 251, loss = 0.15978764\n",
            "Iteration 252, loss = 0.15982554\n",
            "Iteration 253, loss = 0.15837395\n",
            "Iteration 254, loss = 0.15974380\n",
            "Iteration 255, loss = 0.15964844\n",
            "Iteration 256, loss = 0.15781470\n",
            "Iteration 257, loss = 0.15772190\n",
            "Iteration 258, loss = 0.15896239\n",
            "Iteration 259, loss = 0.15842641\n",
            "Iteration 260, loss = 0.15842659\n",
            "Iteration 261, loss = 0.15680391\n",
            "Iteration 262, loss = 0.15752114\n",
            "Iteration 263, loss = 0.15709297\n",
            "Iteration 264, loss = 0.15724272\n",
            "Iteration 265, loss = 0.15790217\n",
            "Iteration 266, loss = 0.15485853\n",
            "Iteration 267, loss = 0.15665948\n",
            "Iteration 268, loss = 0.15822729\n",
            "Iteration 269, loss = 0.15711963\n",
            "Iteration 270, loss = 0.15586712\n",
            "Iteration 271, loss = 0.15403712\n",
            "Iteration 272, loss = 0.15667092\n",
            "Iteration 273, loss = 0.15517464\n",
            "Iteration 274, loss = 0.15633128\n",
            "Iteration 275, loss = 0.15582551\n",
            "Iteration 276, loss = 0.15724734\n",
            "Iteration 277, loss = 0.15481253\n",
            "Iteration 278, loss = 0.15527795\n",
            "Iteration 279, loss = 0.15364963\n",
            "Iteration 280, loss = 0.15470363\n",
            "Iteration 281, loss = 0.15371909\n",
            "Iteration 282, loss = 0.15526307\n",
            "Iteration 283, loss = 0.15443627\n",
            "Iteration 284, loss = 0.15496987\n",
            "Iteration 285, loss = 0.15382998\n",
            "Iteration 286, loss = 0.15309414\n",
            "Iteration 287, loss = 0.15402386\n",
            "Iteration 288, loss = 0.15301328\n",
            "Iteration 289, loss = 0.15451111\n",
            "Iteration 290, loss = 0.15436676\n",
            "Iteration 291, loss = 0.15380921\n",
            "Iteration 292, loss = 0.15236575\n",
            "Iteration 293, loss = 0.15287722\n",
            "Iteration 294, loss = 0.15353306\n",
            "Iteration 295, loss = 0.15316344\n",
            "Iteration 296, loss = 0.15366050\n",
            "Iteration 297, loss = 0.15222835\n",
            "Iteration 298, loss = 0.15081487\n",
            "Iteration 299, loss = 0.15320773\n",
            "Iteration 300, loss = 0.15140230\n",
            "Iteration 301, loss = 0.15087129\n",
            "Iteration 302, loss = 0.15134547\n",
            "Iteration 303, loss = 0.15321693\n",
            "Iteration 304, loss = 0.15116181\n",
            "Iteration 305, loss = 0.15096553\n",
            "Iteration 306, loss = 0.15015566\n",
            "Iteration 307, loss = 0.15075344\n",
            "Iteration 308, loss = 0.15231366\n",
            "Iteration 309, loss = 0.15189624\n",
            "Iteration 310, loss = 0.15045338\n",
            "Iteration 311, loss = 0.15064319\n",
            "Iteration 312, loss = 0.14958437\n",
            "Iteration 313, loss = 0.15142918\n",
            "Iteration 314, loss = 0.14978374\n",
            "Iteration 315, loss = 0.15188972\n",
            "Iteration 316, loss = 0.14951802\n",
            "Iteration 317, loss = 0.14972914\n",
            "Iteration 318, loss = 0.15056459\n",
            "Iteration 319, loss = 0.14875702\n",
            "Iteration 320, loss = 0.14939879\n",
            "Iteration 321, loss = 0.14773793\n",
            "Iteration 322, loss = 0.14749394\n",
            "Iteration 323, loss = 0.14771585\n",
            "Iteration 324, loss = 0.15000969\n",
            "Iteration 325, loss = 0.14948714\n",
            "Iteration 326, loss = 0.14869192\n",
            "Iteration 327, loss = 0.14967152\n",
            "Iteration 328, loss = 0.14747756\n",
            "Iteration 329, loss = 0.14931117\n",
            "Iteration 330, loss = 0.14800305\n",
            "Iteration 331, loss = 0.15023266\n",
            "Iteration 332, loss = 0.14704200\n",
            "Iteration 333, loss = 0.14969475\n",
            "Iteration 334, loss = 0.14562914\n",
            "Iteration 335, loss = 0.14641929\n",
            "Iteration 336, loss = 0.14626416\n",
            "Iteration 337, loss = 0.14776969\n",
            "Iteration 338, loss = 0.14577703\n",
            "Iteration 339, loss = 0.14538108\n",
            "Iteration 340, loss = 0.14582695\n",
            "Iteration 341, loss = 0.14613746\n",
            "Iteration 342, loss = 0.14593866\n",
            "Iteration 343, loss = 0.14723846\n",
            "Iteration 344, loss = 0.14622201\n",
            "Iteration 345, loss = 0.14537539\n",
            "Iteration 346, loss = 0.14483015\n",
            "Iteration 347, loss = 0.14589877\n",
            "Iteration 348, loss = 0.14585142\n",
            "Iteration 349, loss = 0.14661568\n",
            "Iteration 350, loss = 0.14482895\n",
            "Iteration 351, loss = 0.14438651\n",
            "Iteration 352, loss = 0.14422745\n",
            "Iteration 353, loss = 0.14555575\n",
            "Iteration 354, loss = 0.14535190\n",
            "Iteration 355, loss = 0.14454442\n",
            "Iteration 356, loss = 0.14407507\n",
            "Iteration 357, loss = 0.14346078\n",
            "Iteration 358, loss = 0.14543097\n",
            "Iteration 359, loss = 0.14549602\n",
            "Iteration 360, loss = 0.14706821\n",
            "Iteration 361, loss = 0.14482945\n",
            "Iteration 362, loss = 0.14625948\n",
            "Iteration 363, loss = 0.14427598\n",
            "Iteration 364, loss = 0.14382404\n",
            "Iteration 365, loss = 0.14224881\n",
            "Iteration 366, loss = 0.14246582\n",
            "Iteration 367, loss = 0.14306678\n",
            "Iteration 368, loss = 0.14244235\n",
            "Iteration 369, loss = 0.14251008\n",
            "Iteration 370, loss = 0.14377834\n",
            "Iteration 371, loss = 0.14268177\n",
            "Iteration 372, loss = 0.14305180\n",
            "Iteration 373, loss = 0.14295966\n",
            "Iteration 374, loss = 0.14396575\n",
            "Iteration 375, loss = 0.14183105\n",
            "Iteration 376, loss = 0.14376997\n",
            "Iteration 377, loss = 0.14176247\n",
            "Iteration 378, loss = 0.14301283\n",
            "Iteration 379, loss = 0.14163798\n",
            "Iteration 380, loss = 0.14110309\n",
            "Iteration 381, loss = 0.14375899\n",
            "Iteration 382, loss = 0.14272398\n",
            "Iteration 383, loss = 0.14239901\n",
            "Iteration 384, loss = 0.14183037\n",
            "Iteration 385, loss = 0.14002293\n",
            "Iteration 386, loss = 0.14138636\n",
            "Iteration 387, loss = 0.14111667\n",
            "Iteration 388, loss = 0.14297131\n",
            "Iteration 389, loss = 0.14252039\n",
            "Iteration 390, loss = 0.13985808\n",
            "Iteration 391, loss = 0.14005317\n",
            "Iteration 392, loss = 0.14085850\n",
            "Iteration 393, loss = 0.14108225\n",
            "Iteration 394, loss = 0.13954912\n",
            "Iteration 395, loss = 0.14106318\n",
            "Iteration 396, loss = 0.13971294\n",
            "Iteration 397, loss = 0.14069982\n",
            "Iteration 398, loss = 0.14034981\n",
            "Iteration 399, loss = 0.13937838\n",
            "Iteration 400, loss = 0.13833125\n",
            "Iteration 401, loss = 0.14129778\n",
            "Iteration 402, loss = 0.14071776\n",
            "Iteration 403, loss = 0.14044377\n",
            "Iteration 404, loss = 0.14012004\n",
            "Iteration 405, loss = 0.14117737\n",
            "Iteration 406, loss = 0.13944908\n",
            "Iteration 407, loss = 0.13844967\n",
            "Iteration 408, loss = 0.13890049\n",
            "Iteration 409, loss = 0.14021963\n",
            "Iteration 410, loss = 0.13938040\n",
            "Iteration 411, loss = 0.13966423\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)"
            ]
          },
          "execution_count": 313,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 108 -> 55 -> 55 -> 1\n",
        "rede_neural_census = MLPClassifier(verbose=True, max_iter = 1000, tol=0.000010,\n",
        "                                  hidden_layer_sizes = (55,55))\n",
        "rede_neural_census.fit(X_census_treinamento, y_census_treinamento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pvpRABIruivw",
        "outputId": "9be10151-eb62-4060-c4cb-7f70cebc4ea3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
              "      dtype='<U6')"
            ]
          },
          "execution_count": 314,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previsoes = rede_neural_census.predict(X_census_teste)\n",
        "previsoes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uDjNigIBuo-q",
        "outputId": "2c9d538d-096a-4f73-cbe7-7d4a9488c2d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 315,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_census_teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b5tVNTyWuu__",
        "outputId": "22ed09bc-c8c4-4771-fcf6-4d79a07f9240"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8075742067553736"
            ]
          },
          "execution_count": 316,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(y_census_teste, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X4B5vm_rvCcb",
        "outputId": "1102735e-d8c3-4ae7-f899-86fa3a54e5aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8075742067553736"
            ]
          },
          "execution_count": 317,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(rede_neural_census)\n",
        "cm.fit(X_census_treinamento, y_census_treinamento)\n",
        "cm.score(X_census_teste, y_census_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rw-WFDUpv2xe",
        "outputId": "4309cfa4-91c7-4d65-ff17-89b5ee9a07b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.87      0.87      3693\n",
            "        >50K       0.60      0.62      0.61      1192\n",
            "\n",
            "    accuracy                           0.81      4885\n",
            "   macro avg       0.74      0.74      0.74      4885\n",
            "weighted avg       0.81      0.81      0.81      4885\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_census_teste, previsoes))"
      ]
    }
  ]
}